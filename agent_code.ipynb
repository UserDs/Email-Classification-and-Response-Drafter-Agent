{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNVrSyqqp+7OmaHBnbzaepS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ajj_Jaf9y2xm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f233088-c00f-4b87-888e-ffe57a81cc9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.6.2)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.28)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.72)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.6.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.2.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.97.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.3.0,>=0.2.0->langgraph) (3.11.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import TypedDict, List, Dict, Any, Optional\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage"
      ],
      "metadata": {
        "id": "ZKl-F30HOYEQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")"
      ],
      "metadata": {
        "id": "QMC99IyjTeS9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_key\"\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "kUFwYOcoThUT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmailState(TypedDict):\n",
        "    # The email being processed\n",
        "    email: Dict[str, Any]  # Contains subject, sender, body, etc.\n",
        "\n",
        "    # Category of the email (inquiry, complaint, etc.)\n",
        "    email_category: Optional[str]\n",
        "\n",
        "    # Reason why the email was marked as spam\n",
        "    spam_reason: Optional[str]\n",
        "\n",
        "    # Analysis and decisions\n",
        "    is_spam: Optional[bool]\n",
        "\n",
        "    # Response generation\n",
        "    email_draft: Optional[str]\n",
        "\n",
        "    # Processing metadata\n",
        "    messages: List[Dict[str, Any]]  # Track conversation with LLM for analysis"
      ],
      "metadata": {
        "id": "dmnpCX8RTj5a"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize our LLM\n",
        "model = ChatOpenAI(temperature=0)\n",
        "\n",
        "def read_email(state: EmailState):\n",
        "    \"\"\"Alfred reads and logs the incoming email\"\"\"\n",
        "    email = state[\"email\"]\n",
        "\n",
        "    # Here we might do some initial preprocessing\n",
        "    print(f\"Alfred is processing an email from {email['sender']} with subject: {email['subject']}\")\n",
        "\n",
        "    # No state changes needed here\n",
        "    return {}\n",
        "\n",
        "def classify_email(state: EmailState):\n",
        "    \"\"\"Alfred uses an LLM to determine if the email is spam or legitimate\"\"\"\n",
        "    email = state[\"email\"]\n",
        "\n",
        "    # Prepare our prompt for the LLM\n",
        "    prompt = f\"\"\"\n",
        "    As Alfred the butler, analyze this email and determine if it is spam or legitimate.\n",
        "\n",
        "    Email:\n",
        "    From: {email['sender']}\n",
        "    Subject: {email['subject']}\n",
        "    Body: {email['body']}\n",
        "\n",
        "    First, determine if this email is spam. If it is spam, explain why.\n",
        "    If it is legitimate, categorize it (inquiry, complaint, thank you, etc.).\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the LLM\n",
        "    messages = [HumanMessage(content=prompt)]\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # Simple logic to parse the response (in a real app, you'd want more robust parsing)\n",
        "    response_text = response.content.lower()\n",
        "    is_spam = \"spam\" in response_text and \"not spam\" not in response_text\n",
        "\n",
        "    # Extract a reason if it's spam\n",
        "    spam_reason = None\n",
        "    if is_spam and \"reason:\" in response_text:\n",
        "        spam_reason = response_text.split(\"reason:\")[1].strip()\n",
        "\n",
        "    # Determine category if legitimate\n",
        "    email_category = None\n",
        "    if not is_spam:\n",
        "        categories = [\"inquiry\", \"complaint\", \"thank you\", \"request\", \"information\"]\n",
        "        for category in categories:\n",
        "            if category in response_text:\n",
        "                email_category = category\n",
        "                break\n",
        "\n",
        "    # Update messages for tracking\n",
        "    new_messages = state.get(\"messages\", []) + [\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": response.content}\n",
        "    ]\n",
        "\n",
        "    # Return state updates\n",
        "    return {\n",
        "        \"is_spam\": is_spam,\n",
        "        \"spam_reason\": spam_reason,\n",
        "        \"email_category\": email_category,\n",
        "        \"messages\": new_messages\n",
        "    }\n",
        "\n",
        "def handle_spam(state: EmailState):\n",
        "    \"\"\"Alfred discards spam email with a note\"\"\"\n",
        "    print(f\"Alfred has marked the email as spam. Reason: {state['spam_reason']}\")\n",
        "    print(\"The email has been moved to the spam folder.\")\n",
        "\n",
        "    # We're done processing this email\n",
        "    return {}\n",
        "\n",
        "def draft_response(state: EmailState):\n",
        "    \"\"\"Alfred drafts a preliminary response for legitimate emails\"\"\"\n",
        "    email = state[\"email\"]\n",
        "    category = state[\"email_category\"] or \"general\"\n",
        "\n",
        "    # Prepare our prompt for the LLM\n",
        "    prompt = f\"\"\"\n",
        "    As Alfred the butler, draft a polite preliminary response to this email.\n",
        "\n",
        "    Email:\n",
        "    From: {email['sender']}\n",
        "    Subject: {email['subject']}\n",
        "    Body: {email['body']}\n",
        "\n",
        "    This email has been categorized as: {category}\n",
        "\n",
        "    Draft a brief, professional response that Mr. Hugg can review and personalize before sending.\n",
        "    \"\"\"\n",
        "\n",
        "    # Call the LLM\n",
        "    messages = [HumanMessage(content=prompt)]\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # Update messages for tracking\n",
        "    new_messages = state.get(\"messages\", []) + [\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": response.content}\n",
        "    ]\n",
        "\n",
        "    # Return state updates\n",
        "    return {\n",
        "        \"email_draft\": response.content,\n",
        "        \"messages\": new_messages\n",
        "    }\n",
        "\n",
        "def notify_mr_hugg(state: EmailState):\n",
        "    \"\"\"Alfred notifies Mr. Hugg about the email and presents the draft response\"\"\"\n",
        "    email = state[\"email\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Sir, you've received an email from {email['sender']}.\")\n",
        "    print(f\"Subject: {email['subject']}\")\n",
        "    print(f\"Category: {state['email_category']}\")\n",
        "    print(\"\\nI've prepared a draft response for your review:\")\n",
        "    print(\"-\"*50)\n",
        "    print(state[\"email_draft\"])\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # We're done processing this email\n",
        "    return {}"
      ],
      "metadata": {
        "id": "fGDh9rEbTlvO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def route_email(state: EmailState) -> str:\n",
        "    \"\"\"Determine the next step based on spam classification\"\"\"\n",
        "    if state[\"is_spam\"]:\n",
        "        return \"spam\"\n",
        "    else:\n",
        "        return \"legitimate\""
      ],
      "metadata": {
        "id": "U37sB7ICTqx-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the graph\n",
        "email_graph = StateGraph(EmailState)\n",
        "\n",
        "# Add nodes\n",
        "email_graph.add_node(\"read_email\", read_email)\n",
        "email_graph.add_node(\"classify_email\", classify_email)\n",
        "email_graph.add_node(\"handle_spam\", handle_spam)\n",
        "email_graph.add_node(\"draft_response\", draft_response)\n",
        "email_graph.add_node(\"notify_mr_hugg\", notify_mr_hugg)\n",
        "\n",
        "# Start the edges\n",
        "email_graph.add_edge(START, \"read_email\")\n",
        "# Add edges - defining the flow\n",
        "email_graph.add_edge(\"read_email\", \"classify_email\")\n",
        "\n",
        "# Add conditional branching from classify_email\n",
        "email_graph.add_conditional_edges(\n",
        "    \"classify_email\",\n",
        "    route_email,\n",
        "    {\n",
        "        \"spam\": \"handle_spam\",\n",
        "        \"legitimate\": \"draft_response\"\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add the final edges\n",
        "email_graph.add_edge(\"handle_spam\", END)\n",
        "email_graph.add_edge(\"draft_response\", \"notify_mr_hugg\")\n",
        "email_graph.add_edge(\"notify_mr_hugg\", END)\n",
        "\n",
        "# Compile the graph\n",
        "compiled_graph = email_graph.compile()"
      ],
      "metadata": {
        "id": "4Z_gq_qyTtd8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example legitimate email\n",
        "legitimate_email = {\n",
        "    \"sender\": \"john.smith@example.com\",\n",
        "    \"subject\": \"Question about your services\",\n",
        "    \"body\": \"Dear Mr. Hugg, I was referred to you by a colleague and I'm interested in learning more about your consulting services. Could we schedule a call next week? Best regards, John Smith\"\n",
        "}\n",
        "\n",
        "# Example spam email\n",
        "spam_email = {\n",
        "    \"sender\": \"winner@lottery-intl.com\",\n",
        "    \"subject\": \"YOU HAVE WON $5,000,000!!!\",\n",
        "    \"body\": \"CONGRATULATIONS! You have been selected as the winner of our international lottery! To claim your $5,000,000 prize, please send us your bank details and a processing fee of $100.\"\n",
        "}\n",
        "\n",
        "# Process the legitimate email\n",
        "print(\"\\nProcessing legitimate email...\")\n",
        "legitimate_result = compiled_graph.invoke({\n",
        "    \"email\": legitimate_email,\n",
        "    \"is_spam\": None,\n",
        "    \"spam_reason\": None,\n",
        "    \"email_category\": None,\n",
        "    \"email_draft\": None,\n",
        "    \"messages\": []\n",
        "})\n",
        "\n",
        "# Process the spam email\n",
        "print(\"\\nProcessing spam email...\")\n",
        "spam_result = compiled_graph.invoke({\n",
        "    \"email\": spam_email,\n",
        "    \"is_spam\": None,\n",
        "    \"spam_reason\": None,\n",
        "    \"email_category\": None,\n",
        "    \"email_draft\": None,\n",
        "    \"messages\": []\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXIdMZGKTtWx",
        "outputId": "b3cda483-e335-4122-dde4-4eafad966632"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing legitimate email...\n",
            "Alfred is processing an email from john.smith@example.com with subject: Question about your services\n",
            "Alfred has marked the email as spam. Reason: None\n",
            "The email has been moved to the spam folder.\n",
            "\n",
            "Processing spam email...\n",
            "Alfred is processing an email from winner@lottery-intl.com with subject: YOU HAVE WON $5,000,000!!!\n",
            "Alfred has marked the email as spam. Reason: None\n",
            "The email has been moved to the spam folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compiled_graph.get_graph().draw_mermaid()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "-ORpCCQlT6U4",
        "outputId": "5b99a3a9-d4c9-478e-803d-133bdb748c13"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'---\\nconfig:\\n  flowchart:\\n    curve: linear\\n---\\ngraph TD;\\n\\t__start__([<p>__start__</p>]):::first\\n\\tread_email(read_email)\\n\\tclassify_email(classify_email)\\n\\thandle_spam(handle_spam)\\n\\tdraft_response(draft_response)\\n\\tnotify_mr_hugg(notify_mr_hugg)\\n\\t__end__([<p>__end__</p>]):::last\\n\\t__start__ --> read_email;\\n\\tclassify_email -. &nbsp;legitimate&nbsp; .-> draft_response;\\n\\tclassify_email -. &nbsp;spam&nbsp; .-> handle_spam;\\n\\tdraft_response --> notify_mr_hugg;\\n\\tread_email --> classify_email;\\n\\thandle_spam --> __end__;\\n\\tnotify_mr_hugg --> __end__;\\n\\tclassDef default fill:#f2f0ff,line-height:1.2\\n\\tclassDef first fill-opacity:0\\n\\tclassDef last fill:#bfb6fc\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compiled_graph.get_graph().draw_mermaid_png()\n",
        "\n",
        "# To just view the Mermaid text:\n",
        "print(compiled_graph.get_graph().draw_mermaid())\n",
        "\n",
        "# To visualize inside a notebook (Mermaid supported environment):\n",
        "from IPython.display import Markdown\n",
        "display(Markdown(f\"```mermaid\\n{compiled_graph.get_graph().draw_mermaid()}\\n```\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "id": "TLR4rT9dTyUW",
        "outputId": "652d15a5-a403-49d4-d9de-3be475605fb3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "config:\n",
            "  flowchart:\n",
            "    curve: linear\n",
            "---\n",
            "graph TD;\n",
            "\t__start__([<p>__start__</p>]):::first\n",
            "\tread_email(read_email)\n",
            "\tclassify_email(classify_email)\n",
            "\thandle_spam(handle_spam)\n",
            "\tdraft_response(draft_response)\n",
            "\tnotify_mr_hugg(notify_mr_hugg)\n",
            "\t__end__([<p>__end__</p>]):::last\n",
            "\t__start__ --> read_email;\n",
            "\tclassify_email -. &nbsp;legitimate&nbsp; .-> draft_response;\n",
            "\tclassify_email -. &nbsp;spam&nbsp; .-> handle_spam;\n",
            "\tdraft_response --> notify_mr_hugg;\n",
            "\tread_email --> classify_email;\n",
            "\thandle_spam --> __end__;\n",
            "\tnotify_mr_hugg --> __end__;\n",
            "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
            "\tclassDef first fill-opacity:0\n",
            "\tclassDef last fill:#bfb6fc\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```mermaid\n---\nconfig:\n  flowchart:\n    curve: linear\n---\ngraph TD;\n\t__start__([<p>__start__</p>]):::first\n\tread_email(read_email)\n\tclassify_email(classify_email)\n\thandle_spam(handle_spam)\n\tdraft_response(draft_response)\n\tnotify_mr_hugg(notify_mr_hugg)\n\t__end__([<p>__end__</p>]):::last\n\t__start__ --> read_email;\n\tclassify_email -. &nbsp;legitimate&nbsp; .-> draft_response;\n\tclassify_email -. &nbsp;spam&nbsp; .-> handle_spam;\n\tdraft_response --> notify_mr_hugg;\n\tread_email --> classify_email;\n\thandle_spam --> __end__;\n\tnotify_mr_hugg --> __end__;\n\tclassDef default fill:#f2f0ff,line-height:1.2\n\tclassDef first fill-opacity:0\n\tclassDef last fill:#bfb6fc\n\n```"
          },
          "metadata": {}
        }
      ]
    }
  ]
}